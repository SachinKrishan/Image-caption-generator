# Image-caption-generator
In this project, we explored an approach to image captioning using LSTM and CNN models. Our experiments demonstrated that the combination of these models effectively captures the visual and linguistic features necessary for generating accurate and descriptive captions. By leveraging the hierarchical structure of LSTM and the powerful visual representation capabilities of CNN, we achieved comparable results to state-of-the-art models on the Flickr 8k dataset.

Furthermore, we explored the impact of various architectural modifications using different CNN models to generate a vector representation of the images. We used pre-trained state-of-the-art CNN models, including VGG16, ResNet50, and Xception. These experiments showed that Resnet50 is the best CNN for this architecture giving a BLEU-1 and BLEU-2 scores of 0.562 and 0.339, respectively. While our approach shows promising results, there are several avenues for future improvement in this project such as, incorporating attention mechanisms into our model. This could allow it to focus on specific regions of an image while generating captions, potentially improving the alignment between the visual and textual information.
